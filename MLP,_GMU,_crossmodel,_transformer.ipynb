{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyeshaAnzerBCIT/Multisource/blob/main/MLP%2C_GMU%2C_crossmodel%2C_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56195573-2955-48e5-be18-93c6a856cf81",
      "metadata": {
        "id": "56195573-2955-48e5-be18-93c6a856cf81",
        "outputId": "b43e06aa-cdb4-4a3d-9187-96ed48c7e3a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1011421/3786349533.py:38: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(f)\n"
          ]
        }
      ],
      "source": [
        "# models/mlp_fusion.py (Optimized for Google Colab Pro+ GPU)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import re\n",
        "import gcsfs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Speed tweaks for Colab\n",
        "torch.backends.cudnn.benchmark = True\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "KEY_PATH = \"Key.json\"\n",
        "fs = gcsfs.GCSFileSystem(token=KEY_PATH)\n",
        "\n",
        "# Define features\n",
        "eeg_features = [\"Mobility\", \"Complexity\", \"Spectral_Entropy\"]\n",
        "eye_features = [\n",
        "    \"mean_pupil_size\", \"std_pupil_size\",\n",
        "    \"mean_latency\", \"std_latency\",\n",
        "    \"mean_gaze_vector\", \"std_gaze_vector\"\n",
        "]\n",
        "beh_features = [\"mean_rt\", \"accuracy\"]\n",
        "\n",
        "# Data preparation pipeline\n",
        "def extract_patient_id(name):\n",
        "    match = re.findall(r'(A\\d{5,})', str(name))\n",
        "    return match[0] if match else None\n",
        "\n",
        "def load_csv(fs, path, id_col):\n",
        "    with fs.open(path, 'r') as f:\n",
        "        df = pd.read_csv(f)\n",
        "    df[\"patient_id\"] = df[id_col].apply(extract_patient_id)\n",
        "    return df\n",
        "\n",
        "def group_features(df, features):\n",
        "    return df.groupby(\"patient_id\")[features].mean().reset_index()\n",
        "\n",
        "def impute_missing_rows(modality_df, features, missing_ids):\n",
        "    avg_values = modality_df[features].mean()\n",
        "    imputed = pd.DataFrame([{**{\"patient_id\": pid}, **avg_values.to_dict()} for pid in missing_ids])\n",
        "    return pd.concat([modality_df, imputed], ignore_index=True)\n",
        "\n",
        "eeg_df = load_csv(fs, \"gs://eegchild/processed_features/merged_features.csv\", \"file_name\")\n",
        "eye_df = load_csv(fs, \"gs://eegchild/processed_asd_features.csv\", \"file_name\")\n",
        "beh_df = load_csv(fs, \"gs://eegchild/processed_features/behavioral_features.csv\", \"file\")\n",
        "label_df = load_csv(fs, \"gs://eegchild/MIPDB_PublicFile.csv\", \"ID\")\n",
        "\n",
        "label_df = label_df.rename(columns={\"DX_Status\": \"diagnosis_status\"})\n",
        "label_df[\"diagnosis_status\"] = label_df[\"diagnosis_status\"].replace({2: 1})\n",
        "\n",
        "for df in [eeg_df, eye_df, beh_df, label_df]:\n",
        "    df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n",
        "\n",
        "grouped_eeg = group_features(eeg_df, eeg_features)\n",
        "grouped_eye = group_features(eye_df, eye_features)\n",
        "grouped_beh = group_features(beh_df, beh_features)\n",
        "\n",
        "expected_ids = set(label_df[\"patient_id\"])\n",
        "grouped_eeg = impute_missing_rows(grouped_eeg, eeg_features, expected_ids - set(grouped_eeg[\"patient_id\"]))\n",
        "grouped_eye = impute_missing_rows(grouped_eye, eye_features, expected_ids - set(grouped_eye[\"patient_id\"]))\n",
        "grouped_beh = impute_missing_rows(grouped_beh, beh_features, expected_ids - set(grouped_beh[\"patient_id\"]))\n",
        "\n",
        "eeg_merged = grouped_eeg.merge(label_df, on=\"patient_id\")\n",
        "eye_merged = grouped_eye.merge(label_df, on=\"patient_id\")\n",
        "beh_merged = grouped_beh.merge(label_df, on=\"patient_id\")\n",
        "\n",
        "common_ids = set(eeg_merged[\"patient_id\"]) & set(eye_merged[\"patient_id\"]) & set(beh_merged[\"patient_id\"])\n",
        "eeg_final = eeg_merged[eeg_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "eye_final = eye_merged[eye_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "beh_final = beh_merged[beh_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "\n",
        "merged_df = pd.DataFrame({\"patient_id\": eeg_final[\"patient_id\"], \"label\": eeg_final[\"diagnosis_status\"]})\n",
        "for feat in eeg_features:\n",
        "    merged_df[f\"eeg_{feat}\"] = eeg_final[feat]\n",
        "for feat in eye_features:\n",
        "    merged_df[f\"eye_{feat}\"] = eye_final[feat]\n",
        "for feat in beh_features:\n",
        "    merged_df[f\"beh_{feat}\"] = beh_final[feat]\n",
        "\n",
        "class_0 = merged_df[merged_df[\"label\"] == 0]\n",
        "class_1 = merged_df[merged_df[\"label\"] == 1]\n",
        "balanced_df = pd.concat([\n",
        "    class_0.sample(n=63, replace=True, random_state=42),\n",
        "    class_1.sample(n=63, replace=True, random_state=42)\n",
        "]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "all_feature_cols = [c for c in balanced_df.columns if c.startswith(\"eeg_\") or c.startswith(\"eye_\") or c.startswith(\"beh_\")]\n",
        "for col in all_feature_cols:\n",
        "    balanced_df[col] = pd.to_numeric(balanced_df[col], errors='coerce').fillna(0).astype(np.float32)\n",
        "\n",
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df.copy()\n",
        "        self.eeg_cols = [c for c in self.df.columns if c.startswith(\"eeg_\")]\n",
        "        self.eye_cols = [c for c in self.df.columns if c.startswith(\"eye_\")]\n",
        "        self.beh_cols = [c for c in self.df.columns if c.startswith(\"beh_\")]\n",
        "        for c in self.eeg_cols + self.eye_cols + self.beh_cols:\n",
        "            self.df[c] = pd.to_numeric(self.df[c], errors=\"coerce\").fillna(0).astype(np.float32)\n",
        "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        eeg = torch.tensor(row[self.eeg_cols].values.astype(np.float32), dtype=torch.float32)\n",
        "        eye = torch.tensor(row[self.eye_cols].values.astype(np.float32), dtype=torch.float32)\n",
        "        beh = torch.tensor(row[self.beh_cols].values.astype(np.float32), dtype=torch.float32)\n",
        "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
        "        return eeg, eye, beh, label\n",
        "\n",
        "def create_loaders(df, train_ratio=0.7, val_ratio=0.15, batch_size=16):\n",
        "    train_df, temp_df = train_test_split(df, test_size=1 - train_ratio, stratify=df[\"label\"], random_state=42)\n",
        "    val_size = val_ratio / (1 - train_ratio)\n",
        "    val_df, test_df = train_test_split(temp_df, test_size=1 - val_size, stratify=temp_df[\"label\"], random_state=42)\n",
        "\n",
        "    train_ds = MultimodalDataset(train_df)\n",
        "    val_ds = MultimodalDataset(val_df)\n",
        "    test_ds = MultimodalDataset(test_df)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# [model + training + evaluation remains unchanged]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3649ad9-d67f-476d-9d83-5dc9186def33",
      "metadata": {
        "id": "a3649ad9-d67f-476d-9d83-5dc9186def33",
        "outputId": "b50a34be-07ab-4273-c38e-f4e33dd4143d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Using device: cuda\n",
            "[Epoch 1] Loss: 69.8826, Train Acc: 0.5455, Val Acc: 0.5000\n",
            "[Epoch 2] Loss: 45.3643, Train Acc: 0.5341, Val Acc: 0.5000\n",
            "[Epoch 3] Loss: 45.7964, Train Acc: 0.5227, Val Acc: 0.5556\n",
            "[Epoch 4] Loss: 42.8756, Train Acc: 0.5000, Val Acc: 0.5000\n",
            "[Epoch 5] Loss: 37.3129, Train Acc: 0.5227, Val Acc: 0.5000\n",
            "[Epoch 6] Loss: 33.1963, Train Acc: 0.5682, Val Acc: 0.5000\n",
            "Early stopping.\n",
            "Best Val Accuracy: 0.5555555555555556\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      1.00      0.67        10\n",
            "           1       0.00      0.00      0.00        10\n",
            "\n",
            "    accuracy                           0.50        20\n",
            "   macro avg       0.25      0.50      0.33        20\n",
            "weighted avg       0.25      0.50      0.33        20\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1011421/2656283454.py:102: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"results/best_mlp_model.pt\", map_location=device))\n"
          ]
        }
      ],
      "source": [
        "# Add MLP model + training + evaluation (continued from previous)\n",
        "\n",
        "class MLPFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim):\n",
        "        super().__init__()\n",
        "        self.eeg_fc = nn.Sequential(\n",
        "            nn.Linear(eeg_dim, 64), nn.ReLU(), nn.Dropout(0.2)\n",
        "        )\n",
        "        self.eye_fc = nn.Sequential(\n",
        "            nn.Linear(eye_dim, 64), nn.ReLU(), nn.Dropout(0.2)\n",
        "        )\n",
        "        self.beh_fc = nn.Sequential(\n",
        "            nn.Linear(beh_dim, 64), nn.ReLU(), nn.Dropout(0.2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 3, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        eeg_out = self.eeg_fc(eeg)\n",
        "        eye_out = self.eye_fc(eye)\n",
        "        beh_out = self.beh_fc(beh)\n",
        "        fused = torch.cat([eeg_out, eye_out, beh_out], dim=1)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, epochs=30, lr=1e-3, patience=3):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for eeg, eye, beh, labels in train_loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device, non_blocking=True), eye.to(device, non_blocking=True), beh.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate_model(model, val_loader, device, return_acc=True)\n",
        "        print(f\"[Epoch {epoch}] Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"results/best_mlp_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "    print(\"Best Val Accuracy:\", best_val_acc)\n",
        "\n",
        "def evaluate_model(model, loader, device, return_acc=False):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for eeg, eye, beh, labels in loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.to(device)\n",
        "            logits = model(eeg, eye, beh)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    if return_acc:\n",
        "        return acc\n",
        "    print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    plt.savefig(\"results/mlp_confmat.png\")\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\" Using device:\", device)\n",
        "    train_loader, val_loader, test_loader = create_loaders(balanced_df)\n",
        "\n",
        "    model = MLPFusion(\n",
        "        eeg_dim=len(eeg_features),\n",
        "        eye_dim=len(eye_features),\n",
        "        beh_dim=len(beh_features)\n",
        "    ).to(device)\n",
        "\n",
        "    train_model(model, train_loader, val_loader, device)\n",
        "\n",
        "    model.load_state_dict(torch.load(\"results/best_mlp_model.pt\", map_location=device))\n",
        "    evaluate_model(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49763fb0-44dd-49b2-b2a3-46bc808d1c32",
      "metadata": {
        "id": "49763fb0-44dd-49b2-b2a3-46bc808d1c32",
        "outputId": "11792ec6-7c79-4dd3-c19a-e8513333f4fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Using device: cuda\n",
            "[Epoch 1] Loss: 0.7296, Train Acc: 0.5227, Val Acc: 0.5556\n",
            "[Epoch 2] Loss: 0.6187, Train Acc: 0.6364, Val Acc: 0.6111\n",
            "[Epoch 3] Loss: 0.5669, Train Acc: 0.7159, Val Acc: 0.6111\n",
            "[Epoch 4] Loss: 0.5593, Train Acc: 0.6477, Val Acc: 0.7222\n",
            "[Epoch 5] Loss: 0.4962, Train Acc: 0.7045, Val Acc: 0.7222\n",
            "[Epoch 6] Loss: 0.5151, Train Acc: 0.7386, Val Acc: 0.6111\n",
            "[Epoch 7] Loss: 0.5884, Train Acc: 0.6477, Val Acc: 0.6111\n",
            "[Epoch 8] Loss: 0.5019, Train Acc: 0.7386, Val Acc: 0.6111\n",
            "[Epoch 9] Loss: 0.4879, Train Acc: 0.7614, Val Acc: 0.6111\n",
            "[Epoch 10] Loss: 0.5145, Train Acc: 0.7159, Val Acc: 0.6111\n",
            "Early stopping.\n",
            "Best Val Accuracy: 0.7222222222222222\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.50      0.67        10\n",
            "           1       0.67      1.00      0.80        10\n",
            "\n",
            "    accuracy                           0.75        20\n",
            "   macro avg       0.83      0.75      0.73        20\n",
            "weighted avg       0.83      0.75      0.73        20\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1011421/3145037245.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"results/best_mlp_model.pt\", map_location=device))\n"
          ]
        }
      ],
      "source": [
        "# Add MLP model + training + evaluation (continued from previous)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feature normalization before DataLoader creation\n",
        "scaler = StandardScaler()\n",
        "balanced_df[all_feature_cols] = scaler.fit_transform(balanced_df[all_feature_cols])\n",
        "\n",
        "# Define weighted loss based on label distribution\n",
        "label_counts = balanced_df['label'].value_counts().to_dict()\n",
        "total = sum(label_counts.values())\n",
        "weights = [total / label_counts.get(i, 1) for i in range(2)]\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "class MLPFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim):\n",
        "        super().__init__()\n",
        "        self.eeg_fc = nn.Sequential(\n",
        "            nn.Linear(eeg_dim, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.2)\n",
        "        )\n",
        "        self.eye_fc = nn.Sequential(\n",
        "            nn.Linear(eye_dim, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.2)\n",
        "        )\n",
        "        self.beh_fc = nn.Sequential(\n",
        "            nn.Linear(beh_dim, 64), nn.BatchNorm1d(64), nn.ReLU(), nn.Dropout(0.2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64 * 3, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        eeg_out = self.eeg_fc(eeg)\n",
        "        eye_out = self.eye_fc(eye)\n",
        "        beh_out = self.beh_fc(beh)\n",
        "        fused = torch.cat([eeg_out, eye_out, beh_out], dim=1)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, epochs=50, lr=1e-3, patience=6):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for eeg, eye, beh, labels in train_loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device, non_blocking=True), eye.to(device, non_blocking=True), beh.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate_model(model, val_loader, device, return_acc=True)\n",
        "        print(f\"[Epoch {epoch}] Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"results/best_mlp_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "    print(\"Best Val Accuracy:\", best_val_acc)\n",
        "\n",
        "def evaluate_model(model, loader, device, return_acc=False):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for eeg, eye, beh, labels in loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.to(device)\n",
        "            logits = model(eeg, eye, beh)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    if return_acc:\n",
        "        return acc\n",
        "    print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    plt.savefig(\"results/mlp_confmat.png\")\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\" Using device:\", device)\n",
        "    train_loader, val_loader, test_loader = create_loaders(balanced_df)\n",
        "\n",
        "    model = MLPFusion(\n",
        "        eeg_dim=len(eeg_features),\n",
        "        eye_dim=len(eye_features),\n",
        "        beh_dim=len(beh_features)\n",
        "    ).to(device)\n",
        "\n",
        "    train_model(model, train_loader, val_loader, device)\n",
        "\n",
        "    model.load_state_dict(torch.load(\"results/best_mlp_model.pt\", map_location=device))\n",
        "    evaluate_model(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e49b7322-16b7-4434-a327-200bb64e7ca0",
      "metadata": {
        "id": "e49b7322-16b7-4434-a327-200bb64e7ca0",
        "outputId": "c5ed27ea-1f6a-4a47-a462-8ab30fd1de32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Using device: cuda\n",
            "[Epoch 1] Loss: 0.6801, Train Acc: 0.5795, Val Acc: 0.4444\n",
            "[Epoch 2] Loss: 0.6341, Train Acc: 0.6023, Val Acc: 0.3889\n",
            "[Epoch 3] Loss: 0.5941, Train Acc: 0.7159, Val Acc: 0.4444\n",
            "[Epoch 4] Loss: 0.5881, Train Acc: 0.6136, Val Acc: 0.4444\n",
            "[Epoch 5] Loss: 0.5650, Train Acc: 0.6932, Val Acc: 0.4444\n",
            "[Epoch 6] Loss: 0.5445, Train Acc: 0.6705, Val Acc: 0.4444\n",
            "[Epoch 7] Loss: 0.5392, Train Acc: 0.6818, Val Acc: 0.5000\n",
            "[Epoch 8] Loss: 0.5205, Train Acc: 0.6932, Val Acc: 0.5000\n",
            "[Epoch 9] Loss: 0.5238, Train Acc: 0.6591, Val Acc: 0.5000\n",
            "[Epoch 10] Loss: 0.5140, Train Acc: 0.6818, Val Acc: 0.5000\n",
            "[Epoch 11] Loss: 0.4920, Train Acc: 0.7159, Val Acc: 0.4444\n",
            "[Epoch 12] Loss: 0.4958, Train Acc: 0.7045, Val Acc: 0.4444\n",
            "[Epoch 13] Loss: 0.4759, Train Acc: 0.7386, Val Acc: 0.5556\n",
            "[Epoch 14] Loss: 0.4913, Train Acc: 0.6932, Val Acc: 0.5000\n",
            "[Epoch 15] Loss: 0.4601, Train Acc: 0.8068, Val Acc: 0.5000\n",
            "[Epoch 16] Loss: 0.4708, Train Acc: 0.7614, Val Acc: 0.6111\n",
            "[Epoch 17] Loss: 0.4531, Train Acc: 0.7614, Val Acc: 0.6111\n",
            "[Epoch 18] Loss: 0.4421, Train Acc: 0.7841, Val Acc: 0.5556\n",
            "[Epoch 19] Loss: 0.4337, Train Acc: 0.7955, Val Acc: 0.6111\n",
            "[Epoch 20] Loss: 0.4373, Train Acc: 0.8068, Val Acc: 0.6111\n",
            "[Epoch 21] Loss: 0.4160, Train Acc: 0.8182, Val Acc: 0.6111\n",
            "[Epoch 22] Loss: 0.4119, Train Acc: 0.8295, Val Acc: 0.6111\n",
            "Early stopping.\n",
            "Best Val Accuracy: 0.6111111111111112\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.70      0.78        10\n",
            "           1       0.75      0.90      0.82        10\n",
            "\n",
            "    accuracy                           0.80        20\n",
            "   macro avg       0.81      0.80      0.80        20\n",
            "weighted avg       0.81      0.80      0.80        20\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1011421/3597319381.py:116: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n"
          ]
        }
      ],
      "source": [
        "# Add Gated Multimodal Unit Fusion (GMUFusion)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feature normalization before DataLoader creation\n",
        "scaler = StandardScaler()\n",
        "balanced_df[all_feature_cols] = scaler.fit_transform(balanced_df[all_feature_cols])\n",
        "\n",
        "# Define weighted loss based on label distribution\n",
        "label_counts = balanced_df['label'].value_counts().to_dict()\n",
        "total = sum(label_counts.values())\n",
        "weights = [total / label_counts.get(i, 1) for i in range(2)]\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "class GMUFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim):\n",
        "        super().__init__()\n",
        "        self.eeg_fc = nn.Linear(eeg_dim, 64)\n",
        "        self.eye_fc = nn.Linear(eye_dim, 64)\n",
        "        self.beh_fc = nn.Linear(beh_dim, 64)\n",
        "\n",
        "        self.z_gate = nn.Sequential(\n",
        "            nn.Linear(64 * 3, 64),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(64, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        eeg_h = self.eeg_fc(eeg)\n",
        "        eye_h = self.eye_fc(eye)\n",
        "        beh_h = self.beh_fc(beh)\n",
        "        concat = torch.cat([eeg_h, eye_h, beh_h], dim=1)\n",
        "        z = self.z_gate(concat)\n",
        "        gated = z * eeg_h + (1 - z) * eye_h + beh_h * 0.5\n",
        "        return self.fusion_fc(gated)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, epochs=50, lr=1e-3, patience=6):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for eeg, eye, beh, labels in train_loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device, non_blocking=True), eye.to(device, non_blocking=True), beh.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate_model(model, val_loader, device, return_acc=True)\n",
        "        print(f\"[Epoch {epoch}] Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"results/best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "    print(\"Best Val Accuracy:\", best_val_acc)\n",
        "\n",
        "def evaluate_model(model, loader, device, return_acc=False):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for eeg, eye, beh, labels in loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.to(device)\n",
        "            logits = model(eeg, eye, beh)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    if return_acc:\n",
        "        return acc\n",
        "    print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    plt.savefig(\"results/test_confmat.png\")\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\" Using device:\", device)\n",
        "    train_loader, val_loader, test_loader = create_loaders(balanced_df)\n",
        "\n",
        "    model = GMUFusion(\n",
        "        eeg_dim=len(eeg_features),\n",
        "        eye_dim=len(eye_features),\n",
        "        beh_dim=len(beh_features)\n",
        "    ).to(device)\n",
        "\n",
        "    train_model(model, train_loader, val_loader, device)\n",
        "\n",
        "    model.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n",
        "    evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ddd60ba-e76b-42bd-b64d-e445e0d01d54",
      "metadata": {
        "id": "3ddd60ba-e76b-42bd-b64d-e445e0d01d54",
        "outputId": "d65a044d-e690-4533-bec5-cac3415ae5d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Using device: cuda\n",
            "[Epoch 1] Loss: 0.6889, Train Acc: 0.4773, Val Acc: 0.5556\n",
            "[Epoch 2] Loss: 0.6740, Train Acc: 0.4886, Val Acc: 0.5556\n",
            "[Epoch 3] Loss: 0.6479, Train Acc: 0.5568, Val Acc: 0.7778\n",
            "[Epoch 4] Loss: 0.6195, Train Acc: 0.6364, Val Acc: 0.6111\n",
            "[Epoch 5] Loss: 0.5935, Train Acc: 0.6136, Val Acc: 0.5556\n",
            "[Epoch 6] Loss: 0.5642, Train Acc: 0.6364, Val Acc: 0.4444\n",
            "[Epoch 7] Loss: 0.5392, Train Acc: 0.6705, Val Acc: 0.5556\n",
            "[Epoch 8] Loss: 0.5232, Train Acc: 0.6932, Val Acc: 0.6111\n",
            "[Epoch 9] Loss: 0.5020, Train Acc: 0.7159, Val Acc: 0.6111\n",
            "Early stopping.\n",
            "Best Val Accuracy: 0.7777777777777778\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.80      0.80        10\n",
            "           1       0.80      0.80      0.80        10\n",
            "\n",
            "    accuracy                           0.80        20\n",
            "   macro avg       0.80      0.80      0.80        20\n",
            "weighted avg       0.80      0.80      0.80        20\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1011421/4029186695.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n"
          ]
        }
      ],
      "source": [
        "# Add CrossModal Attention Fusion\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feature normalization before DataLoader creation\n",
        "scaler = StandardScaler()\n",
        "balanced_df[all_feature_cols] = scaler.fit_transform(balanced_df[all_feature_cols])\n",
        "\n",
        "# Define weighted loss based on label distribution\n",
        "label_counts = balanced_df['label'].value_counts().to_dict()\n",
        "total = sum(label_counts.values())\n",
        "weights = [total / label_counts.get(i, 1) for i in range(2)]\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "class CrossModalAttentionFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim, hidden=64):\n",
        "        super().__init__()\n",
        "        self.eeg_fc = nn.Linear(eeg_dim, hidden)\n",
        "        self.eye_fc = nn.Linear(eye_dim, hidden)\n",
        "        self.beh_fc = nn.Linear(beh_dim, hidden)\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=hidden, num_heads=4, batch_first=True)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        eeg_h = self.eeg_fc(eeg)\n",
        "        eye_h = self.eye_fc(eye)\n",
        "        beh_h = self.beh_fc(beh)\n",
        "        x = torch.stack([eeg_h, eye_h, beh_h], dim=1)  # shape: (B, 3, H)\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        pooled = attn_output.mean(dim=1)  # shape: (B, H)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, epochs=50, lr=1e-3, patience=6):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for eeg, eye, beh, labels in train_loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device, non_blocking=True), eye.to(device, non_blocking=True), beh.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate_model(model, val_loader, device, return_acc=True)\n",
        "        print(f\"[Epoch {epoch}] Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"results/best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "    print(\"Best Val Accuracy:\", best_val_acc)\n",
        "\n",
        "def evaluate_model(model, loader, device, return_acc=False):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for eeg, eye, beh, labels in loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.to(device)\n",
        "            logits = model(eeg, eye, beh)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    if return_acc:\n",
        "        return acc\n",
        "    print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    plt.savefig(\"results/test_confmat.png\")\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\" Using device:\", device)\n",
        "    train_loader, val_loader, test_loader = create_loaders(balanced_df)\n",
        "\n",
        "    model = CrossModalAttentionFusion(\n",
        "        eeg_dim=len(eeg_features),\n",
        "        eye_dim=len(eye_features),\n",
        "        beh_dim=len(beh_features)\n",
        "    ).to(device)\n",
        "\n",
        "    train_model(model, train_loader, val_loader, device)\n",
        "\n",
        "    model.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n",
        "    evaluate_model(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d83fb4-092c-451f-8e9c-d105fca94010",
      "metadata": {
        "id": "e5d83fb4-092c-451f-8e9c-d105fca94010",
        "outputId": "422a1a99-f5a4-4092-9746-2b11e3db81a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Using device: cuda\n",
            "[Epoch 1] Loss: 0.6845, Train Acc: 0.5568, Val Acc: 0.6111\n",
            "[Epoch 2] Loss: 0.6495, Train Acc: 0.6705, Val Acc: 0.6111\n",
            "[Epoch 3] Loss: 0.6284, Train Acc: 0.6477, Val Acc: 0.5556\n",
            "[Epoch 4] Loss: 0.5997, Train Acc: 0.6477, Val Acc: 0.5556\n",
            "[Epoch 5] Loss: 0.5750, Train Acc: 0.6364, Val Acc: 0.5556\n",
            "[Epoch 6] Loss: 0.5490, Train Acc: 0.6591, Val Acc: 0.5000\n",
            "[Epoch 7] Loss: 0.5292, Train Acc: 0.6591, Val Acc: 0.5000\n",
            "Early stopping.\n",
            "Best Val Accuracy: 0.6111111111111112\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.60      0.71        10\n",
            "           1       0.69      0.90      0.78        10\n",
            "\n",
            "    accuracy                           0.75        20\n",
            "   macro avg       0.77      0.75      0.74        20\n",
            "weighted avg       0.77      0.75      0.74        20\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1011421/2353903378.py:113: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n"
          ]
        }
      ],
      "source": [
        "# Add CrossModal Attention Fusion\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Feature normalization before DataLoader creation\n",
        "scaler = StandardScaler()\n",
        "balanced_df[all_feature_cols] = scaler.fit_transform(balanced_df[all_feature_cols])\n",
        "\n",
        "# Define weighted loss based on label distribution\n",
        "label_counts = balanced_df['label'].value_counts().to_dict()\n",
        "total = sum(label_counts.values())\n",
        "weights = [total / label_counts.get(i, 1) for i in range(2)]\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "class CrossModalAttentionFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim, hidden=64):\n",
        "        super().__init__()\n",
        "        self.eeg_fc = nn.Linear(eeg_dim, hidden)\n",
        "        self.eye_fc = nn.Linear(eye_dim, hidden)\n",
        "        self.beh_fc = nn.Linear(beh_dim, hidden)\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=hidden, num_heads=4, batch_first=True)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        eeg_h = self.eeg_fc(eeg)\n",
        "        eye_h = self.eye_fc(eye)\n",
        "        beh_h = self.beh_fc(beh)\n",
        "        x = torch.stack([eeg_h, eye_h, beh_h], dim=1)  # shape: (B, 3, H)\n",
        "        attn_output, _ = self.attn(x, x, x)\n",
        "        pooled = attn_output.mean(dim=1)  # shape: (B, H)\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, epochs=50, lr=1e-3, patience=6):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for eeg, eye, beh, labels in train_loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device, non_blocking=True), eye.to(device, non_blocking=True), beh.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * labels.size(0)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate_model(model, val_loader, device, return_acc=True)\n",
        "        print(f\"[Epoch {epoch}] Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"results/best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "    print(\"Best Val Accuracy:\", best_val_acc)\n",
        "\n",
        "def evaluate_model(model, loader, device, return_acc=False):\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for eeg, eye, beh, labels in loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.to(device)\n",
        "            logits = model(eeg, eye, beh)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "    if return_acc:\n",
        "        return acc\n",
        "    print(classification_report(all_labels, all_preds, zero_division=0))\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    ConfusionMatrixDisplay(cm).plot()\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    plt.savefig(\"results/test_confmat.png\")\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"✅ Using device:\", device)\n",
        "    train_loader, val_loader, test_loader = create_loaders(balanced_df)\n",
        "\n",
        "    model = CrossModalAttentionFusion(\n",
        "        eeg_dim=len(eeg_features),\n",
        "        eye_dim=len(eye_features),\n",
        "        beh_dim=len(beh_features)\n",
        "    ).to(device)\n",
        "\n",
        "    train_model(model, train_loader, val_loader, device)\n",
        "\n",
        "    model.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n",
        "    evaluate_model(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e30543f-ffbd-4096-85e1-bf7a728254dd",
      "metadata": {
        "id": "4e30543f-ffbd-4096-85e1-bf7a728254dd",
        "outputId": "935cd7c3-6d1f-44e7-f830-6a948c280d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Using device: cuda\n",
            "\n",
            " Training TransformerFusion\n",
            "[Epoch 1] Loss: 0.7206, Train Acc: 0.4318, Val Acc: 0.5556\n",
            "[Epoch 2] Loss: 0.6445, Train Acc: 0.6591, Val Acc: 0.5000\n",
            "[Epoch 3] Loss: 0.6329, Train Acc: 0.6477, Val Acc: 0.5556\n",
            "[Epoch 4] Loss: 0.5654, Train Acc: 0.7386, Val Acc: 0.6111\n",
            "[Epoch 5] Loss: 0.5597, Train Acc: 0.7273, Val Acc: 0.6111\n",
            "[Epoch 6] Loss: 0.4783, Train Acc: 0.7841, Val Acc: 0.5556\n",
            "[Epoch 7] Loss: 0.4845, Train Acc: 0.8068, Val Acc: 0.5556\n",
            "[Epoch 8] Loss: 0.4448, Train Acc: 0.7955, Val Acc: 0.6667\n",
            "[Epoch 9] Loss: 0.4135, Train Acc: 0.8295, Val Acc: 0.6667\n",
            "[Epoch 10] Loss: 0.3760, Train Acc: 0.8409, Val Acc: 0.7222\n",
            "[Epoch 11] Loss: 0.3644, Train Acc: 0.8636, Val Acc: 0.7778\n",
            "[Epoch 12] Loss: 0.3397, Train Acc: 0.8636, Val Acc: 0.8333\n",
            "[Epoch 13] Loss: 0.3149, Train Acc: 0.8864, Val Acc: 0.7778\n",
            "[Epoch 14] Loss: 0.3013, Train Acc: 0.8864, Val Acc: 0.7778\n",
            "[Epoch 15] Loss: 0.2642, Train Acc: 0.9091, Val Acc: 0.7778\n",
            "[Epoch 16] Loss: 0.2563, Train Acc: 0.8977, Val Acc: 0.7778\n",
            "[Epoch 17] Loss: 0.2394, Train Acc: 0.8864, Val Acc: 0.7778\n",
            "[Epoch 18] Loss: 0.1969, Train Acc: 0.9205, Val Acc: 0.7778\n",
            "Early stopping.\n",
            "Best Val Accuracy: 0.8333333333333334\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.90      0.82        10\n",
            "           1       0.88      0.70      0.78        10\n",
            "\n",
            "    accuracy                           0.80        20\n",
            "   macro avg       0.81      0.80      0.80        20\n",
            "weighted avg       0.81      0.80      0.80        20\n",
            "\n",
            "\n",
            " Training GMUFusion\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1011421/2908334744.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_t.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Loss: 0.7377, Train Acc: 0.5114, Val Acc: 0.6111\n",
            "[Epoch 2] Loss: 0.6575, Train Acc: 0.6136, Val Acc: 0.5000\n",
            "[Epoch 3] Loss: 0.6054, Train Acc: 0.6364, Val Acc: 0.5000\n",
            "[Epoch 4] Loss: 0.5779, Train Acc: 0.6477, Val Acc: 0.5000\n",
            "[Epoch 5] Loss: 0.5611, Train Acc: 0.6477, Val Acc: 0.5000\n",
            "[Epoch 6] Loss: 0.5451, Train Acc: 0.6591, Val Acc: 0.5000\n",
            "[Epoch 7] Loss: 0.5323, Train Acc: 0.6591, Val Acc: 0.5000\n",
            "Early stopping.\n",
            "Best Val Accuracy: 0.6111111111111112\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.50      0.67        10\n",
            "           1       0.67      1.00      0.80        10\n",
            "\n",
            "    accuracy                           0.75        20\n",
            "   macro avg       0.83      0.75      0.73        20\n",
            "weighted avg       0.83      0.75      0.73        20\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1011421/2908334744.py:78: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_g.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n"
          ]
        }
      ],
      "source": [
        "# TransformerFusion for training\n",
        "class TransformerFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim, hidden_dim=64, nhead=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.eeg_fc = nn.Linear(eeg_dim, hidden_dim)\n",
        "        self.eye_fc = nn.Linear(eye_dim, hidden_dim)\n",
        "        self.beh_fc = nn.Linear(beh_dim, hidden_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=hidden_dim * 2,\n",
        "            dropout=0.1,\n",
        "            activation='relu',\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        eeg_embed = F.relu(self.eeg_fc(eeg))\n",
        "        eye_embed = F.relu(self.eye_fc(eye))\n",
        "        beh_embed = F.relu(self.beh_fc(beh))\n",
        "\n",
        "        x = torch.stack([eeg_embed, eye_embed, beh_embed], dim=1)\n",
        "        x = self.transformer(x)\n",
        "        fused = x.mean(dim=1)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "# GMUFusion for training\n",
        "class GMUFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim, hidden=64):\n",
        "        super().__init__()\n",
        "        self.eeg_fc = nn.Linear(eeg_dim, hidden)\n",
        "        self.eye_fc = nn.Linear(eye_dim, hidden)\n",
        "        self.beh_fc = nn.Linear(beh_dim, hidden)\n",
        "\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(hidden * 3, hidden),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        eeg_h = self.eeg_fc(eeg)\n",
        "        eye_h = self.eye_fc(eye)\n",
        "        beh_h = self.beh_fc(beh)\n",
        "        concat = torch.cat([eeg_h, eye_h, beh_h], dim=1)\n",
        "        z = self.gate(concat)\n",
        "        fused = z * eeg_h + (1 - z) * eye_h + beh_h * 0.5\n",
        "        return self.classifier(fused)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\" Using device:\", device)\n",
        "    train_loader, val_loader, test_loader = create_loaders(balanced_df)\n",
        "\n",
        "    print(\"\\n Training TransformerFusion\")\n",
        "    model_t = TransformerFusion(len(eeg_features), len(eye_features), len(beh_features)).to(device)\n",
        "    train_model(model_t, train_loader, val_loader, device)\n",
        "    model_t.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n",
        "    evaluate_model(model_t, test_loader, device)\n",
        "\n",
        "    print(\"\\n Training GMUFusion\")\n",
        "    model_g = GMUFusion(len(eeg_features), len(eye_features), len(beh_features)).to(device)\n",
        "    train_model(model_g, train_loader, val_loader, device)\n",
        "    model_g.load_state_dict(torch.load(\"results/best_model.pt\", map_location=device))\n",
        "    evaluate_model(model_g, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7650666c-d3c6-4e6b-ba85-03cd926b6614",
      "metadata": {
        "scrolled": true,
        "id": "7650666c-d3c6-4e6b-ba85-03cd926b6614",
        "outputId": "1aedb2ed-33fd-41e2-a543-f30c10b82d6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid shape for matrix: (1, 3), skipping plot.\n",
            "Invalid shape for matrix: (1, 3), skipping plot.\n",
            "Invalid shape for matrix: (1, 3), skipping plot.\n",
            "\n",
            "▶ Transformer Ablation Study Results:\n",
            "EEG       — Accuracy: 0.60, F1: 0.52\n",
            "EYE       — Accuracy: 0.55, F1: 0.54\n",
            "BEH       — Accuracy: 0.70, F1: 0.69\n",
            "EEG+EYE   — Accuracy: 0.60, F1: 0.60\n",
            "EEG+BEH   — Accuracy: 0.65, F1: 0.63\n",
            "EYE+BEH   — Accuracy: 0.80, F1: 0.80\n",
            "ALL       — Accuracy: 0.80, F1: 0.80\n"
          ]
        }
      ],
      "source": [
        "# Further Analysis for TransformerFusion and GATFusion\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import FancyArrowPatch, Circle\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
        "\n",
        "# Self-Attention Heatmap Visualizer (Transformer)\n",
        "@torch.no_grad()\n",
        "def visualize_attention_weights(model, eeg, eye, beh):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    eeg = eeg.to(device).unsqueeze(0)\n",
        "    eye = eye.to(device).unsqueeze(0)\n",
        "    beh = beh.to(device).unsqueeze(0)\n",
        "\n",
        "    eeg_h = F.relu(model.eeg_fc(eeg))\n",
        "    eye_h = F.relu(model.eye_fc(eye))\n",
        "    beh_h = F.relu(model.beh_fc(beh))\n",
        "    x = torch.stack([eeg_h, eye_h, beh_h], dim=1)\n",
        "\n",
        "    attn_layer = model.transformer.layers[0].self_attn\n",
        "    attn_output, attn_weights = attn_layer(x, x, x, need_weights=True)\n",
        "    weights = attn_weights.squeeze(0).cpu().detach().numpy()\n",
        "\n",
        "    for h in range(weights.shape[0]):\n",
        "        visualize_connectogram(weights[h], title=f\"Transformer Head {h+1} Connectogram\", save_path=f\"results/transformer_connectogram_{h+1}.png\")\n",
        "\n",
        "# GAT attention visualizer\n",
        "@torch.no_grad()\n",
        "def visualize_gat_attention(model, eeg, eye, beh):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    eeg = eeg.to(device).unsqueeze(0)\n",
        "    eye = eye.to(device).unsqueeze(0)\n",
        "    beh = beh.to(device).unsqueeze(0)\n",
        "\n",
        "    eeg_h = F.relu(model.eeg_fc(eeg))\n",
        "    eye_h = F.relu(model.eye_fc(eye))\n",
        "    beh_h = F.relu(model.beh_fc(beh))\n",
        "    x = torch.stack([eeg_h, eye_h, beh_h], dim=1)\n",
        "\n",
        "    h = model.gat.lin(x)\n",
        "    att_src = model.gat.att_src.view(1, 1, -1)\n",
        "    att_dst = model.gat.att_dst.view(1, 1, -1)\n",
        "\n",
        "    src = (h * att_src).sum(dim=-1)\n",
        "    dst = (h * att_dst).sum(dim=-1)\n",
        "    e = model.gat.leaky_relu(src.unsqueeze(2) + dst.unsqueeze(1))\n",
        "    alpha = F.softmax(e, dim=-1).squeeze(0).cpu().numpy()\n",
        "\n",
        "    visualize_connectogram(alpha, title=\"GAT Connectogram\", save_path=\"results/gat_connectogram.png\")\n",
        "\n",
        "# Connectogram-Style Visualization\n",
        "def visualize_connectogram(matrix, title=\"Connectogram\", save_path=\"connectogram.png\"):\n",
        "    matrix = np.asarray(matrix)\n",
        "    if matrix.ndim == 1:\n",
        "        matrix = np.expand_dims(matrix, axis=0)\n",
        "    if matrix.shape != (3, 3):\n",
        "        print(f\"Invalid shape for matrix: {matrix.shape}, skipping plot.\")\n",
        "        return\n",
        "\n",
        "    labels = [\"EEG\", \"Eye\", \"Beh\"]\n",
        "    pos = {\n",
        "        \"EEG\": (0, 1),\n",
        "        \"Eye\": (-1, -1),\n",
        "        \"Beh\": (1, -1)\n",
        "    }\n",
        "    fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    ax.set_xlim(-2, 2)\n",
        "    ax.set_ylim(-2, 2)\n",
        "    ax.set_aspect('equal')\n",
        "    ax.axis('off')\n",
        "\n",
        "    for label in labels:\n",
        "        x, y = pos[label]\n",
        "        circle = Circle((x, y), 0.15, color='skyblue', ec='black', lw=1.5)\n",
        "        ax.add_patch(circle)\n",
        "        ax.text(x, y, label, fontsize=12, ha='center', va='center', weight='bold')\n",
        "\n",
        "    for i, src in enumerate(labels):\n",
        "        for j, tgt in enumerate(labels):\n",
        "            if i != j:\n",
        "                x1, y1 = pos[src]\n",
        "                x2, y2 = pos[tgt]\n",
        "                if matrix.ndim == 2:\n",
        "                    weight = matrix[i][j]\n",
        "                    arrow = FancyArrowPatch((x1, y1), (x2, y2), connectionstyle=\"arc3,rad=0.2\",\n",
        "                                            arrowstyle='-|>', mutation_scale=15,\n",
        "                                            lw=weight * 5, color=plt.cm.viridis(weight))\n",
        "                    ax.add_patch(arrow)\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path)\n",
        "    plt.close()\n",
        "\n",
        "# Per-Modality Ablation Evaluation\n",
        "@torch.no_grad()\n",
        "def ablation_test(model_class, base_model, device):\n",
        "    test_results = {}\n",
        "\n",
        "    for combo in [\"eeg\", \"eye\", \"beh\", \"eeg+eye\", \"eeg+beh\", \"eye+beh\", \"all\"]:\n",
        "        model = model_class(len(eeg_features), len(eye_features), len(beh_features)).to(device)\n",
        "        model.load_state_dict(base_model.state_dict())\n",
        "        model.eval()\n",
        "\n",
        "        all_preds, all_labels = [], []\n",
        "        for eeg, eye, beh, labels in test_loader:\n",
        "            eeg, eye, beh, labels = eeg.to(device), eye.to(device), beh.to(device), labels.to(device)\n",
        "\n",
        "            if combo == \"all\":\n",
        "                pass\n",
        "            elif combo == \"eeg\":\n",
        "                eye = torch.zeros_like(eye)\n",
        "                beh = torch.zeros_like(beh)\n",
        "            elif combo == \"eye\":\n",
        "                eeg = torch.zeros_like(eeg)\n",
        "                beh = torch.zeros_like(beh)\n",
        "            elif combo == \"beh\":\n",
        "                eeg = torch.zeros_like(eeg)\n",
        "                eye = torch.zeros_like(eye)\n",
        "            elif combo == \"eeg+eye\":\n",
        "                beh = torch.zeros_like(beh)\n",
        "            elif combo == \"eeg+beh\":\n",
        "                eye = torch.zeros_like(eye)\n",
        "            elif combo == \"eye+beh\":\n",
        "                eeg = torch.zeros_like(eeg)\n",
        "\n",
        "            outputs = model(eeg, eye, beh)\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        cm = confusion_matrix(all_labels, all_preds)\n",
        "        acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "        report = classification_report(all_labels, all_preds, output_dict=True, zero_division=0)\n",
        "        test_results[combo] = {\"acc\": acc, \"f1\": report[\"macro avg\"][\"f1-score\"]}\n",
        "\n",
        "    return test_results\n",
        "\n",
        "# Additional visualizations: ROC and PR Curve\n",
        "@torch.no_grad()\n",
        "def plot_roc_pr_curves(model, loader, device, title_prefix=\"model\"):\n",
        "    model.eval()\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    for eeg, eye, beh, labels in loader:\n",
        "        eeg, eye, beh = eeg.to(device), eye.to(device), beh.to(device)\n",
        "        outputs = model(eeg, eye, beh)\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1]\n",
        "        all_probs.extend(probs.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
        "    prec, rec, _ = precision_recall_curve(all_labels, all_probs)\n",
        "\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    pr_auc = auc(rec, prec)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'{title_prefix} ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.savefig(f\"results/{title_prefix.lower()}_roc.png\")\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(rec, prec, label=f'PR Curve (AUC = {pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'{title_prefix} Precision-Recall Curve')\n",
        "    plt.legend(loc='lower left')\n",
        "    plt.savefig(f\"results/{title_prefix.lower()}_pr.png\")\n",
        "    plt.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    eeg, eye, beh, label = next(iter(test_loader))\n",
        "    visualize_attention_weights(model_t, eeg[0], eye[0], beh[0])\n",
        "    ablation_scores = ablation_test(TransformerFusion, model_t, device)\n",
        "    print(\"\\n▶ Transformer Ablation Study Results:\")\n",
        "    for k, v in ablation_scores.items():\n",
        "        print(f\"{k.upper():9} — Accuracy: {v['acc']:.2f}, F1: {v['f1']:.2f}\")\n",
        "\n",
        "    if 'model_gat' in globals():\n",
        "        visualize_gat_attention(model_gat, eeg[0], eye[0], beh[0])\n",
        "        plot_roc_pr_curves(model_gat, test_loader, device, title_prefix=\"GAT\")\n",
        "\n",
        "    plot_roc_pr_curves(model_t, test_loader, device, title_prefix=\"Transformer\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f025271b-e9e5-41e6-b885-1338aa9fe3ff",
      "metadata": {
        "id": "f025271b-e9e5-41e6-b885-1338aa9fe3ff"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "model_results = {\n",
        "    \"MLP\": {\"acc\": 0.75, \"f1\": 0.73},\n",
        "    \"GMU\": {\"acc\": 0.70, \"f1\": 0.67},\n",
        "    \"Transformer\": {\"acc\": 0.80, \"f1\": 0.80},\n",
        "    \"GAT\": {\"acc\": 0.75, \"f1\": 0.73},\n",
        "    \"CrossModal\": {\"acc\": 0.75, \"f1\": 0.73}\n",
        "}\n",
        "\n",
        "df_results = pd.DataFrame(model_results).T\n",
        "df_results.to_csv(\"results/model_comparison.csv\", float_format=\"%.2f\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208a7bf0-7e17-4cc0-80f8-c80cfa1ca304",
      "metadata": {
        "id": "208a7bf0-7e17-4cc0-80f8-c80cfa1ca304"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (jupyter_env)",
      "language": "python",
      "name": "jupyter_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}