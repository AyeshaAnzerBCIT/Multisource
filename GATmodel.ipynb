{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyeshaAnzerBCIT/Multisource/blob/main/GATmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df48793-c7c4-424e-8a0a-617a90d4c88d",
      "metadata": {
        "scrolled": true,
        "id": "4df48793-c7c4-424e-8a0a-617a90d4c88d",
        "outputId": "25847e35-7168-4f47-b125-d49a990b5154"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_324855/3590853156.py:26: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(f)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final balanced dataset shape: (126, 13)\n",
            "Class distribution: label\n",
            "1    63\n",
            "0    63\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Column dtypes after numeric conversion:\n",
            "patient_id               object\n",
            "label                     int64\n",
            "eeg_Mobility            float32\n",
            "eeg_Complexity          float32\n",
            "eeg_Spectral_Entropy    float32\n",
            "eye_mean_pupil_size     float32\n",
            "eye_std_pupil_size      float32\n",
            "eye_mean_latency        float32\n",
            "eye_std_latency         float32\n",
            "eye_mean_gaze_vector    float32\n",
            "eye_std_gaze_vector     float32\n",
            "beh_mean_rt             float32\n",
            "beh_accuracy            float32\n",
            "dtype: object\n",
            "\n",
            "Sample rows:\n",
            "   patient_id  label  eeg_Mobility  eeg_Complexity  eeg_Spectral_Entropy  \\\n",
            "0  A00063558      1      0.192332        2.461543              2.219917   \n",
            "1  A00055956      0      0.192332        2.461543              2.219917   \n",
            "2  A00057599      1      0.192332        2.461543              2.219917   \n",
            "\n",
            "   eye_mean_pupil_size  eye_std_pupil_size  eye_mean_latency  eye_std_latency  \\\n",
            "0            18.102869            5.497564       2262.257812      3805.833008   \n",
            "1            14.546218            6.564981       2136.692383      5807.206543   \n",
            "2             9.539749            5.188206       1879.663208      3705.418701   \n",
            "\n",
            "   eye_mean_gaze_vector  eye_std_gaze_vector  beh_mean_rt  beh_accuracy  \n",
            "0             -0.103817             0.285277         87.0     47.832859  \n",
            "1             -0.111250             0.229144         86.5     50.000000  \n",
            "2             -0.116054             0.221058         87.0     54.400639  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import gcsfs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# GCS authentication\n",
        "KEY_PATH = \"Key.json\"\n",
        "fs = gcsfs.GCSFileSystem(token=KEY_PATH)\n",
        "\n",
        "# Feature columns\n",
        "eeg_features = [\"Mobility\", \"Complexity\", \"Spectral_Entropy\"]\n",
        "eye_features = [\n",
        "    \"mean_pupil_size\", \"std_pupil_size\",\n",
        "    \"mean_latency\", \"std_latency\",\n",
        "    \"mean_gaze_vector\", \"std_gaze_vector\"\n",
        "]\n",
        "beh_features = [\"mean_rt\", \"accuracy\"]\n",
        "\n",
        "def extract_patient_id(name):\n",
        "    match = re.findall(r'(A\\d{5,})', str(name))\n",
        "    return match[0] if match else None\n",
        "\n",
        "def load_csv(fs, path, id_col):\n",
        "    with fs.open(path, 'r') as f:\n",
        "        df = pd.read_csv(f)\n",
        "    df[\"patient_id\"] = df[id_col].apply(extract_patient_id)\n",
        "    return df\n",
        "\n",
        "def group_features(df, features):\n",
        "    return df.groupby(\"patient_id\")[features].mean().reset_index()\n",
        "\n",
        "def impute_missing_rows(modality_df, features, missing_ids):\n",
        "    avg_values = modality_df[features].mean()\n",
        "    imputed = pd.DataFrame([{**{\"patient_id\": pid}, **avg_values.to_dict()} for pid in missing_ids])\n",
        "    return pd.concat([modality_df, imputed], ignore_index=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Load Raw Modality Data from GCS\n",
        "# -----------------------------\n",
        "eeg_df = load_csv(fs, \"gs://eegchild/processed_features/merged_features.csv\", \"file_name\")\n",
        "eye_df = load_csv(fs, \"gs://eegchild/processed_asd_features.csv\", \"file_name\")\n",
        "beh_df = load_csv(fs, \"gs://eegchild/processed_features/behavioral_features.csv\", \"file\")\n",
        "label_df = load_csv(fs, \"gs://eegchild/MIPDB_PublicFile.csv\", \"ID\")\n",
        "\n",
        "# Binarize labels: 0 => no diagnosis, 1 => diagnosis (2 => 1)\n",
        "label_df = label_df.rename(columns={\"DX_Status\": \"diagnosis_status\"})\n",
        "label_df[\"diagnosis_status\"] = label_df[\"diagnosis_status\"].replace({2: 1})\n",
        "\n",
        "# Convert patient IDs to string\n",
        "for df in [eeg_df, eye_df, beh_df, label_df]:\n",
        "    df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Group & Impute Missing\n",
        "# -----------------------------\n",
        "grouped_eeg = group_features(eeg_df, eeg_features)\n",
        "grouped_eye = group_features(eye_df, eye_features)\n",
        "grouped_beh = group_features(beh_df, beh_features)\n",
        "\n",
        "expected_ids = set(label_df[\"patient_id\"])\n",
        "grouped_eeg = impute_missing_rows(grouped_eeg, eeg_features, expected_ids - set(grouped_eeg[\"patient_id\"]))\n",
        "grouped_eye = impute_missing_rows(grouped_eye, eye_features, expected_ids - set(grouped_eye[\"patient_id\"]))\n",
        "grouped_beh = impute_missing_rows(grouped_beh, beh_features, expected_ids - set(grouped_beh[\"patient_id\"]))\n",
        "\n",
        "# Merge each with label info\n",
        "eeg_merged = grouped_eeg.merge(label_df, on=\"patient_id\")\n",
        "eye_merged = grouped_eye.merge(label_df, on=\"patient_id\")\n",
        "beh_merged = grouped_beh.merge(label_df, on=\"patient_id\")\n",
        "\n",
        "# Find common patient IDs\n",
        "common_ids = set(eeg_merged[\"patient_id\"]) & set(eye_merged[\"patient_id\"]) & set(beh_merged[\"patient_id\"])\n",
        "eeg_final = eeg_merged[eeg_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "eye_final = eye_merged[eye_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "beh_final = beh_merged[beh_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Merge into One DataFrame\n",
        "# -----------------------------\n",
        "merged_df = pd.DataFrame({\n",
        "    \"patient_id\": eeg_final[\"patient_id\"],\n",
        "    \"label\": eeg_final[\"diagnosis_status\"]\n",
        "})\n",
        "\n",
        "for feat in eeg_features:\n",
        "    merged_df[f\"eeg_{feat}\"] = eeg_final[feat]\n",
        "for feat in eye_features:\n",
        "    merged_df[f\"eye_{feat}\"] = eye_final[feat]\n",
        "for feat in beh_features:\n",
        "    merged_df[f\"beh_{feat}\"] = beh_final[feat]\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Balance Dataset (63 each)\n",
        "# -----------------------------\n",
        "class_0 = merged_df[merged_df[\"label\"] == 0]\n",
        "class_1 = merged_df[merged_df[\"label\"] == 1]\n",
        "balanced_df = pd.concat([\n",
        "    class_0.sample(n=63, replace=True, random_state=42),\n",
        "    class_1.sample(n=63, replace=True, random_state=42)\n",
        "]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Final balanced dataset shape:\", balanced_df.shape)\n",
        "print(\"Class distribution:\", balanced_df['label'].value_counts())\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Ensure All Feature Columns Are float32\n",
        "# -----------------------------\n",
        "all_feature_cols = [\n",
        "    col for col in balanced_df.columns\n",
        "    if col.startswith(\"eeg_\") or col.startswith(\"eye_\") or col.startswith(\"beh_\")\n",
        "]\n",
        "\n",
        "for c in all_feature_cols:\n",
        "    # Convert forcibly to float32, dropping/coercing non-numerics\n",
        "    balanced_df[c] = pd.to_numeric(balanced_df[c], errors='coerce').fillna(0).astype(np.float32)\n",
        "\n",
        "# (Optional) Drop any leftover non-feature columns if needed\n",
        "extra_cols = [c for c in balanced_df.columns if c not in all_feature_cols + [\"patient_id\", \"label\"]]\n",
        "if extra_cols:\n",
        "    print(\"Dropping leftover columns:\", extra_cols)\n",
        "    balanced_df.drop(columns=extra_cols, inplace=True)\n",
        "\n",
        "# Double-check\n",
        "print(\"\\nColumn dtypes after numeric conversion:\")\n",
        "print(balanced_df.dtypes)\n",
        "\n",
        "print(\"\\nSample rows:\\n\", balanced_df.head(3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2884e18a-e762-4605-81f8-ec166c2a529f",
      "metadata": {
        "id": "2884e18a-e762-4605-81f8-ec166c2a529f",
        "outputId": "252e2cc1-18ae-4ccc-cc73-eaf5d4ec0660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All feature columns:\n",
            "- eeg_Mobility\n",
            "- eeg_Complexity\n",
            "- eeg_Spectral_Entropy\n",
            "- eye_mean_pupil_size\n",
            "- eye_std_pupil_size\n",
            "- eye_mean_latency\n",
            "- eye_std_latency\n",
            "- eye_mean_gaze_vector\n",
            "- eye_std_gaze_vector\n",
            "- beh_mean_rt\n",
            "- beh_accuracy\n",
            "\n",
            "Final check (dtypes):\n",
            "eeg_Mobility            float32\n",
            "eeg_Complexity          float32\n",
            "eeg_Spectral_Entropy    float32\n",
            "eye_mean_pupil_size     float32\n",
            "eye_std_pupil_size      float32\n",
            "eye_mean_latency        float32\n",
            "eye_std_latency         float32\n",
            "eye_mean_gaze_vector    float32\n",
            "eye_std_gaze_vector     float32\n",
            "beh_mean_rt             float32\n",
            "beh_accuracy            float32\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Identify all feature columns from the balanced DataFrame\n",
        "all_feature_cols = []\n",
        "for col in balanced_df.columns:\n",
        "    if col.startswith(\"eeg_\") or col.startswith(\"eye_\") or col.startswith(\"beh_\"):\n",
        "        all_feature_cols.append(col)\n",
        "\n",
        "print(\"All feature columns:\")\n",
        "for c in all_feature_cols:\n",
        "    print(\"-\", c)\n",
        "\n",
        "# Example numeric conversion step (optional) to guarantee float32\n",
        "for c in all_feature_cols:\n",
        "    balanced_df[c] = pd.to_numeric(balanced_df[c], errors='coerce').fillna(0).astype('float32')\n",
        "\n",
        "# Now you have a list of your final numeric features\n",
        "print(\"\\nFinal check (dtypes):\")\n",
        "print(balanced_df[all_feature_cols].dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f16f86b-ddb0-4323-8a1d-662a8bb99f52",
      "metadata": {
        "id": "0f16f86b-ddb0-4323-8a1d-662a8bb99f52"
      },
      "outputs": [],
      "source": [
        "# part2_gat_model.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Minimal GAT-like layer for a 3-node graph (EEG, Eye, Beh).\n",
        "    \"\"\"\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        self.att_src = nn.Parameter(torch.zeros((1, 1, out_dim)))\n",
        "        self.att_dst = nn.Parameter(torch.zeros((1, 1, out_dim)))\n",
        "        nn.init.xavier_uniform_(self.lin.weight, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.att_src, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.att_dst, gain=1.414)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          x: shape (B, N=3, in_dim)\n",
        "        Returns:\n",
        "          shape (B, N=3, out_dim)\n",
        "        \"\"\"\n",
        "        B, N, _ = x.shape\n",
        "        h = self.lin(x)  # (B, N, out_dim)\n",
        "\n",
        "        src = (h * self.att_src).sum(dim=-1).unsqueeze(2)  # (B, N, 1)\n",
        "        dst = (h * self.att_dst).sum(dim=-1).unsqueeze(1)  # (B, 1, N)\n",
        "        e = self.leaky_relu(src + dst)  # (B, N, N)\n",
        "\n",
        "        alpha = F.softmax(e, dim=-1)     # attention across the N nodes\n",
        "        out = torch.bmm(alpha, h)        # (B, N, out_dim)\n",
        "        return F.elu(out)                # final activation\n",
        "\n",
        "class GATFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    GAT-based fusion of EEG, Eye, and Behavioral modalities.\n",
        "    Each modality is encoded to hidden_dim, then the GAT layer merges them.\n",
        "    \"\"\"\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim, hidden_dim=64, out_dim=64):\n",
        "        super().__init__()\n",
        "        # Per-modality FC encoders\n",
        "        self.eeg_fc = nn.Linear(eeg_dim, hidden_dim)\n",
        "        self.eye_fc = nn.Linear(eye_dim, hidden_dim)\n",
        "        self.beh_fc = nn.Linear(beh_dim, hidden_dim)\n",
        "\n",
        "        # Graph Attention\n",
        "        self.gat = GATLayer(in_dim=hidden_dim, out_dim=out_dim)\n",
        "\n",
        "        # Final classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(out_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)  # binary classification\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        # (B, eeg_dim) => (B, hidden_dim)\n",
        "        eeg_embed = F.relu(self.eeg_fc(eeg))\n",
        "        eye_embed = F.relu(self.eye_fc(eye))\n",
        "        beh_embed = F.relu(self.beh_fc(beh))\n",
        "\n",
        "        # Stack: (B, 3, hidden_dim)\n",
        "        stack = torch.stack([eeg_embed, eye_embed, beh_embed], dim=1)\n",
        "\n",
        "        # GAT => (B, 3, out_dim)\n",
        "        gat_out = self.gat(stack)\n",
        "        fused = gat_out.mean(dim=1)  # (B, out_dim)\n",
        "\n",
        "        return self.classifier(fused)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00d8fde1-04a6-4766-a926-a318aa986112",
      "metadata": {
        "id": "00d8fde1-04a6-4766-a926-a318aa986112",
        "outputId": "55ae0f80-d799-4237-fd18-fb44fbc7324b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_324855/3621124811.py:28: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(f)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced shape: (126, 13)\n",
            "Class distribution: label\n",
            "1    63\n",
            "0    63\n",
            "Name: count, dtype: int64\n",
            "Check dtypes:\n",
            " eeg_Mobility            float32\n",
            "eeg_Complexity          float32\n",
            "eeg_Spectral_Entropy    float32\n",
            "eye_mean_pupil_size     float32\n",
            "eye_std_pupil_size      float32\n",
            "eye_mean_latency        float32\n",
            "eye_std_latency         float32\n",
            "eye_mean_gaze_vector    float32\n",
            "eye_std_gaze_vector     float32\n",
            "beh_mean_rt             float32\n",
            "beh_accuracy            float32\n",
            "dtype: object\n",
            "Train/Val/Test sizes: 88, 18, 20\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "84",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 84",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[26], line 310\u001b[0m\n\u001b[1;32m    301\u001b[0m model \u001b[38;5;241m=\u001b[39m GATFusion(\n\u001b[1;32m    302\u001b[0m     eeg_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(eeg_features),\n\u001b[1;32m    303\u001b[0m     eye_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(eye_features),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    306\u001b[0m     out_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\n\u001b[1;32m    307\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m \u001b[43mtrain_gat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Evaluate best model on test set\u001b[39;00m\n\u001b[1;32m    313\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults/gat_best_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice))\n",
            "Cell \u001b[0;32mIn[26], line 232\u001b[0m, in \u001b[0;36mtrain_gat_model\u001b[0;34m(model, train_loader, val_loader, device, epochs, lr, patience)\u001b[0m\n\u001b[1;32m    229\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    230\u001b[0m correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eeg, eye, beh, labels, pid \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m    233\u001b[0m     eeg, eye, beh, labels \u001b[38;5;241m=\u001b[39m eeg\u001b[38;5;241m.\u001b[39mto(device), eye\u001b[38;5;241m.\u001b[39mto(device), beh\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    234\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[0;32mIn[26], line 185\u001b[0m, in \u001b[0;36mMultimodalGATDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# use .loc and cast to the right dtype; avoids the object-array problem\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     eeg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meeg_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m    186\u001b[0m     eye \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meye_cols]\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[1;32m    187\u001b[0m     beh \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39mloc[idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeh_cols]\u001b[38;5;241m.\u001b[39mto_numpy(dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32))\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/pandas/core/indexing.py:1184\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1186\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/pandas/core/indexing.py:1368\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(IndexingError):\n\u001b[1;32m   1367\u001b[0m     tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_ellipsis(tup)\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_lowerdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;66;03m# no multi-index, so validate all of the indexers\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m tup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tuple_indexer(tup)\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/pandas/core/indexing.py:1065\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tup):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_label_like(key):\n\u001b[1;32m   1063\u001b[0m         \u001b[38;5;66;03m# We don't need to check for tuples here because those are\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m         \u001b[38;5;66;03m#  caught by the _is_nested_tuple_indexer check above.\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m         section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# We should never have a scalar section here, because\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m#  _getitem_lowerdim is only called after a check for\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m#  is_scalar_access, which that would be.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m section\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim:\n\u001b[1;32m   1071\u001b[0m             \u001b[38;5;66;03m# we're in the middle of slicing through a MultiIndex\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m             \u001b[38;5;66;03m# revise the key wrt to `section` by inserting an _NS\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/pandas/core/indexing.py:1431\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;66;03m# fall thru to straight lookup\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[0;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/pandas/core/indexing.py:1381\u001b[0m, in \u001b[0;36m_LocIndexer._get_label\u001b[0;34m(self, label, axis)\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_label\u001b[39m(\u001b[38;5;28mself\u001b[39m, label, axis: AxisInt):\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;66;03m# GH#5567 this will fail if the label is not present in the axis.\u001b[39;00m\n\u001b[0;32m-> 1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/pandas/core/generic.py:4301\u001b[0m, in \u001b[0;36mNDFrame.xs\u001b[0;34m(self, key, axis, level, drop_level)\u001b[0m\n\u001b[1;32m   4299\u001b[0m             new_index \u001b[38;5;241m=\u001b[39m index[loc]\n\u001b[1;32m   4300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4301\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m   4304\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m loc\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mbool_:\n",
            "File \u001b[0;32m~/miniconda3/envs/jupyter_env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[0;31mKeyError\u001b[0m: 84"
          ]
        }
      ],
      "source": [
        "###############################################\n",
        "# Part 1: Load & Balance Data from GCS\n",
        "###############################################\n",
        "\n",
        "import os\n",
        "import re\n",
        "import gcsfs\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "KEY_PATH = \"Key.json\"\n",
        "fs = gcsfs.GCSFileSystem(token=KEY_PATH)\n",
        "\n",
        "eeg_features = [\"Mobility\", \"Complexity\", \"Spectral_Entropy\"]\n",
        "eye_features = [\n",
        "    \"mean_pupil_size\", \"std_pupil_size\",\n",
        "    \"mean_latency\", \"std_latency\",\n",
        "    \"mean_gaze_vector\", \"std_gaze_vector\"\n",
        "]\n",
        "beh_features = [\"mean_rt\", \"accuracy\"]\n",
        "\n",
        "def extract_patient_id(name):\n",
        "    match = re.findall(r'(A\\d{5,})', str(name))\n",
        "    return match[0] if match else None\n",
        "\n",
        "def load_csv(fs, path, id_col):\n",
        "    with fs.open(path, 'r') as f:\n",
        "        df = pd.read_csv(f)\n",
        "    df[\"patient_id\"] = df[id_col].apply(extract_patient_id)\n",
        "    return df\n",
        "\n",
        "def group_features(df, features):\n",
        "    return df.groupby(\"patient_id\")[features].mean().reset_index()\n",
        "\n",
        "def impute_missing_rows(modality_df, features, missing_ids):\n",
        "    avg_values = modality_df[features].mean()\n",
        "    imputed = pd.DataFrame([{**{\"patient_id\": pid}, **avg_values.to_dict()} for pid in missing_ids])\n",
        "    return pd.concat([modality_df, imputed], ignore_index=True)\n",
        "\n",
        "# 1) Load\n",
        "eeg_df = load_csv(fs, \"gs://eegchild/processed_features/merged_features.csv\", \"file_name\")\n",
        "eye_df = load_csv(fs, \"gs://eegchild/processed_asd_features.csv\", \"file_name\")\n",
        "beh_df = load_csv(fs, \"gs://eegchild/processed_features/behavioral_features.csv\", \"file\")\n",
        "label_df = load_csv(fs, \"gs://eegchild/MIPDB_PublicFile.csv\", \"ID\")\n",
        "\n",
        "label_df = label_df.rename(columns={\"DX_Status\": \"diagnosis_status\"})\n",
        "label_df[\"diagnosis_status\"] = label_df[\"diagnosis_status\"].replace({2: 1})\n",
        "\n",
        "for df in [eeg_df, eye_df, beh_df, label_df]:\n",
        "    df[\"patient_id\"] = df[\"patient_id\"].astype(str)\n",
        "\n",
        "# 2) Group & Impute\n",
        "grouped_eeg = group_features(eeg_df, eeg_features)\n",
        "grouped_eye = group_features(eye_df, eye_features)\n",
        "grouped_beh = group_features(beh_df, beh_features)\n",
        "\n",
        "expected_ids = set(label_df[\"patient_id\"])\n",
        "grouped_eeg = impute_missing_rows(grouped_eeg, eeg_features, expected_ids - set(grouped_eeg[\"patient_id\"]))\n",
        "grouped_eye = impute_missing_rows(grouped_eye, eye_features, expected_ids - set(grouped_eye[\"patient_id\"]))\n",
        "grouped_beh = impute_missing_rows(grouped_beh, beh_features, expected_ids - set(grouped_beh[\"patient_id\"]))\n",
        "\n",
        "# Merge label\n",
        "eeg_merged = grouped_eeg.merge(label_df, on=\"patient_id\")\n",
        "eye_merged = grouped_eye.merge(label_df, on=\"patient_id\")\n",
        "beh_merged = grouped_beh.merge(label_df, on=\"patient_id\")\n",
        "\n",
        "# 3) Filter to common patients\n",
        "common_ids = set(eeg_merged[\"patient_id\"]) & set(eye_merged[\"patient_id\"]) & set(beh_merged[\"patient_id\"])\n",
        "eeg_final = eeg_merged[eeg_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "eye_final = eye_merged[eye_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "beh_final = beh_merged[beh_merged[\"patient_id\"].isin(common_ids)].reset_index(drop=True)\n",
        "\n",
        "# 4) Merge into one DataFrame\n",
        "merged_df = pd.DataFrame({\"patient_id\": eeg_final[\"patient_id\"], \"label\": eeg_final[\"diagnosis_status\"]})\n",
        "for feat in eeg_features:\n",
        "    merged_df[f\"eeg_{feat}\"] = eeg_final[feat]\n",
        "for feat in eye_features:\n",
        "    merged_df[f\"eye_{feat}\"] = eye_final[feat]\n",
        "for feat in beh_features:\n",
        "    merged_df[f\"beh_{feat}\"] = beh_final[feat]\n",
        "\n",
        "# 5) Balance classes to 63 each\n",
        "class_0 = merged_df[merged_df[\"label\"] == 0]\n",
        "class_1 = merged_df[merged_df[\"label\"] == 1]\n",
        "balanced_df = pd.concat([\n",
        "    class_0.sample(n=63, replace=True, random_state=42),\n",
        "    class_1.sample(n=63, replace=True, random_state=42)\n",
        "]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Balanced shape:\", balanced_df.shape)\n",
        "print(\"Class distribution:\", balanced_df[\"label\"].value_counts())\n",
        "\n",
        "# Force numeric float32 for all feature columns\n",
        "all_feature_cols = []\n",
        "for c in balanced_df.columns:\n",
        "    if c.startswith(\"eeg_\") or c.startswith(\"eye_\") or c.startswith(\"beh_\"):\n",
        "        all_feature_cols.append(c)\n",
        "\n",
        "for col in all_feature_cols:\n",
        "    balanced_df[col] = pd.to_numeric(balanced_df[col], errors='coerce').fillna(0).astype(np.float32)\n",
        "\n",
        "print(\"Check dtypes:\\n\", balanced_df[all_feature_cols].dtypes)\n",
        "\n",
        "\n",
        "###############################################\n",
        "# Part 2: GATFusion Model\n",
        "###############################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GATLayer(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_dim, out_dim, bias=False)\n",
        "        self.att_src = nn.Parameter(torch.zeros((1, 1, out_dim)))\n",
        "        self.att_dst = nn.Parameter(torch.zeros((1, 1, out_dim)))\n",
        "        nn.init.xavier_uniform_(self.lin.weight, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.att_src, gain=1.414)\n",
        "        nn.init.xavier_uniform_(self.att_dst, gain=1.414)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, _ = x.shape\n",
        "        h = self.lin(x)\n",
        "        src = (h * self.att_src).sum(dim=-1).unsqueeze(2)\n",
        "        dst = (h * self.att_dst).sum(dim=-1).unsqueeze(1)\n",
        "        e = self.leaky_relu(src + dst)  # (B, N, N)\n",
        "\n",
        "        alpha = F.softmax(e, dim=-1)\n",
        "        out = torch.bmm(alpha, h)  # (B, N, out_dim)\n",
        "        return F.elu(out)\n",
        "\n",
        "class GATFusion(nn.Module):\n",
        "    def __init__(self, eeg_dim, eye_dim, beh_dim, hidden_dim=64, out_dim=64):\n",
        "        super().__init__()\n",
        "        self.eeg_fc = nn.Linear(eeg_dim, hidden_dim)\n",
        "        self.eye_fc = nn.Linear(eye_dim, hidden_dim)\n",
        "        self.beh_fc = nn.Linear(beh_dim, hidden_dim)\n",
        "        self.gat = GATLayer(hidden_dim, out_dim)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(out_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, eeg, eye, beh):\n",
        "        eeg_embed = F.relu(self.eeg_fc(eeg))\n",
        "        eye_embed = F.relu(self.eye_fc(eye))\n",
        "        beh_embed = F.relu(self.beh_fc(beh))\n",
        "        stack = torch.stack([eeg_embed, eye_embed, beh_embed], dim=1)\n",
        "        gat_out = self.gat(stack)\n",
        "        fused = gat_out.mean(dim=1)\n",
        "        return self.classifier(fused)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2096528e-e5b3-4392-bc68-03ba8cd2f7f8",
      "metadata": {
        "id": "2096528e-e5b3-4392-bc68-03ba8cd2f7f8"
      },
      "outputs": [],
      "source": [
        "###############################################\n",
        "# Part 3: Dataset + DataLoader  (final patch)\n",
        "###############################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class MultimodalGATDataset(Dataset):\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        # Keep original index; we'll access rows by position (iloc)\n",
        "        self.df = df.copy()\n",
        "\n",
        "        # Column groups for each modality\n",
        "        self.eeg_cols = [c for c in self.df.columns if c.startswith(\"eeg_\")]\n",
        "        self.eye_cols = [c for c in self.df.columns if c.startswith(\"eye_\")]\n",
        "        self.beh_cols = [c for c in self.df.columns if c.startswith(\"beh_\")]\n",
        "\n",
        "        # Ensure every feature is numeric float32\n",
        "        for col in self.eeg_cols + self.eye_cols + self.beh_cols:\n",
        "            self.df[col] = (\n",
        "                pd.to_numeric(self.df[col], errors=\"coerce\")\n",
        "                .fillna(0)\n",
        "                .astype(np.float32)\n",
        "            )\n",
        "\n",
        "        self.df[\"label\"] = self.df[\"label\"].astype(int)\n",
        "        if \"patient_id\" not in self.df.columns:\n",
        "            self.df[\"patient_id\"] = [\"unknown\"] * len(self.df)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        eeg   = torch.from_numpy(row[self.eeg_cols].values.astype(np.float32))\n",
        "        eye   = torch.from_numpy(row[self.eye_cols].values.astype(np.float32))\n",
        "        beh   = torch.from_numpy(row[self.beh_cols].values.astype(np.float32))\n",
        "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
        "        pid   = row[\"patient_id\"]\n",
        "\n",
        "        return eeg, eye, beh, label, pid\n",
        "\n",
        "\n",
        "def standardise_split(\n",
        "    train_df: pd.DataFrame,\n",
        "    *other_dfs: pd.DataFrame,\n",
        "    feature_cols: list[str],\n",
        "):\n",
        "    \"\"\"\n",
        "    Z‑score every feature using statistics computed **only** on train_df.\n",
        "    Returns the transformed DataFrames and a dict of means/stds.\n",
        "    \"\"\"\n",
        "    stats = defaultdict(dict)\n",
        "    for col in feature_cols:\n",
        "        mu  = train_df[col].mean()\n",
        "        sig = train_df[col].std() if train_df[col].std() > 0 else 1.0  # avoid /0\n",
        "        stats[col][\"mean\"] = mu\n",
        "        stats[col][\"std\"]  = sig\n",
        "\n",
        "        train_df[col] = (train_df[col] - mu) / sig\n",
        "        for df in other_dfs:\n",
        "            df[col] = (df[col] - mu) / sig\n",
        "\n",
        "    return (train_df, *other_dfs), stats\n",
        "\n",
        "\n",
        "def create_gat_loaders(\n",
        "    df: pd.DataFrame,\n",
        "    train_ratio: float = 0.7,\n",
        "    val_ratio: float   = 0.15,\n",
        "    batch_size: int    = 16,\n",
        "):\n",
        "    # Stratified train/val/test split\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df,\n",
        "        test_size=1 - train_ratio,\n",
        "        stratify=df[\"label\"],\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    val_size = val_ratio / (1 - train_ratio)\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=1 - val_size,\n",
        "        stratify=temp_df[\"label\"],\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "    # ── Feature standardisation (train stats only) ──\n",
        "    feature_cols = [c for c in df.columns if c.startswith((\"eeg_\", \"eye_\", \"beh_\"))]\n",
        "    (train_df, val_df, test_df), _ = standardise_split(\n",
        "        train_df, val_df, test_df, feature_cols=feature_cols\n",
        "    )\n",
        "\n",
        "    # Build datasets & loaders\n",
        "    train_ds = MultimodalGATDataset(train_df)\n",
        "    val_ds   = MultimodalGATDataset(val_df)\n",
        "    test_ds  = MultimodalGATDataset(test_df)\n",
        "\n",
        "    print(f\"Train/Val/Test sizes: {len(train_ds)}, {len(val_ds)}, {len(test_ds)}\")\n",
        "\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=batch_size, shuffle=True),\n",
        "        DataLoader(val_ds,   batch_size=batch_size),\n",
        "        DataLoader(test_ds,  batch_size=batch_size),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ee66c65-2010-41e1-9183-ccf0af72f9ca",
      "metadata": {
        "id": "3ee66c65-2010-41e1-9183-ccf0af72f9ca"
      },
      "outputs": [],
      "source": [
        "###############################################\n",
        "# Part 4: Training + Evaluation  (quick‑fix v2)\n",
        "###############################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Binary / multi‑class focal loss.\n",
        "    γ: focusing parameter (γ = 2 is common)\n",
        "    α: weighting tensor for classes, shape = [C]\n",
        "    \"\"\"\n",
        "    def __init__(self, alpha=None, gamma=2.0, reduction=\"mean\"):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha  # torch tensor or None\n",
        "        self.reduction = reduction\n",
        "        self.ce = nn.CrossEntropyLoss(weight=alpha, reduction=\"none\")\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce_loss = self.ce(logits, targets)\n",
        "        pt = torch.exp(-ce_loss)            # prob of the true class\n",
        "        focal = (1 - pt) ** self.gamma * ce_loss\n",
        "        if self.reduction == \"mean\":\n",
        "            return focal.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            return focal.sum()\n",
        "        return focal\n",
        "\n",
        "\n",
        "def train_gat_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        device,\n",
        "        *,\n",
        "        epochs: int = 50,\n",
        "        lr: float = 5e-4,\n",
        "        patience: int = 5,\n",
        "        use_focal: bool = False,        # ← toggle focal loss\n",
        "        class0_weight: float = 1.1,     # ← gentler bias toward class 0\n",
        "):\n",
        "    # ── 1. Build loss function ───────────────────────────────────────────────\n",
        "    alpha = torch.tensor([class0_weight, 1.5], dtype=torch.float32).to(device)\n",
        "    criterion = (\n",
        "        FocalLoss(alpha=alpha, gamma=1.0)\n",
        "        if use_focal\n",
        "        else nn.CrossEntropyLoss(weight=alpha)\n",
        "    )\n",
        "\n",
        "    # ── 2. Optimiser ─────────────────────────────────────────────────────────\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val = 0.0\n",
        "    patience_cnt = 0\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # ── train one epoch ──────────────────────────────────────────────────\n",
        "        model.train()\n",
        "        run_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for eeg, eye, beh, labels, _ in train_loader:\n",
        "            eeg, eye, beh, labels = (\n",
        "                eeg.to(device), eye.to(device), beh.to(device), labels.to(device)\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(eeg, eye, beh)\n",
        "            loss = criterion(out, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            run_loss += loss.item() * labels.size(0)\n",
        "            preds = out.argmax(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = run_loss / total\n",
        "        train_acc = correct / total\n",
        "        val_acc = evaluate_gat_model(model, val_loader, device, return_acc=True)\n",
        "\n",
        "        print(f\"[Ep {epoch:2d}/{epochs}] \"\n",
        "              f\"Loss {train_loss:.4f} | Train Acc {train_acc:.4f} | Val Acc {val_acc:.4f}\")\n",
        "\n",
        "        # ── early stopping ───────────────────────────────────────────────────\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            patience_cnt = 0\n",
        "            os.makedirs(\"results\", exist_ok=True)\n",
        "            torch.save(model.state_dict(), \"results/gat_best_model.pt\")\n",
        "            print(\"  Best model updated!\")\n",
        "        else:\n",
        "            patience_cnt += 1\n",
        "            if patience_cnt >= patience:\n",
        "                print(\"  Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "    print(f\"\\nBest Validation Accuracy: {best_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0d250c5-34d1-418a-913b-981e1cc62a2a",
      "metadata": {
        "id": "e0d250c5-34d1-418a-913b-981e1cc62a2a",
        "outputId": "f4069616-5f38-4432-ec8e-9192536f5a0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train/Val/Test sizes: 88, 18, 20\n",
            "[Ep  1/50] Loss 0.4728 | Train Acc 0.5795 | Val Acc 0.4444\n",
            "  Best model updated!\n",
            "[Ep  2/50] Loss 0.4810 | Train Acc 0.5568 | Val Acc 0.5000\n",
            "  Best model updated!\n",
            "[Ep  3/50] Loss 0.4799 | Train Acc 0.6136 | Val Acc 0.5000\n",
            "[Ep  4/50] Loss 0.4769 | Train Acc 0.6023 | Val Acc 0.5000\n",
            "[Ep  5/50] Loss 0.4762 | Train Acc 0.6023 | Val Acc 0.5000\n",
            "[Ep  6/50] Loss 0.4761 | Train Acc 0.5455 | Val Acc 0.5000\n",
            "[Ep  7/50] Loss 0.4569 | Train Acc 0.6364 | Val Acc 0.5000\n",
            "  Early stopping triggered.\n",
            "\n",
            "Best Validation Accuracy: 0.5000\n",
            "\n",
            "--- Evaluation Metrics ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.10      0.18        10\n",
            "           1       0.53      1.00      0.69        10\n",
            "\n",
            "    accuracy                           0.55        20\n",
            "   macro avg       0.76      0.55      0.44        20\n",
            "weighted avg       0.76      0.55      0.44        20\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_324855/2781501100.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"results/gat_best_model.pt\", map_location=device))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "###############################################\n",
        "# USAGE:\n",
        "###############################################\n",
        "if __name__ == \"__main__\":\n",
        "    import torch\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Now that balanced_df is produced, create loaders\n",
        "    train_loader, val_loader, test_loader = create_gat_loaders(balanced_df)\n",
        "\n",
        "    # Initialize model\n",
        "    model = GATFusion(\n",
        "        eeg_dim=len(eeg_features),\n",
        "        eye_dim=len(eye_features),\n",
        "        beh_dim=len(beh_features),\n",
        "        hidden_dim=64,\n",
        "        out_dim=64\n",
        "    ).to(device)\n",
        "\n",
        "    # Train\n",
        "    train_gat_model(model, train_loader, val_loader, device, epochs=50, lr=5e-5, patience=5, use_focal = True, class0_weight=1.0)\n",
        "\n",
        "    # Evaluate best model on test set\n",
        "    model.load_state_dict(torch.load(\"results/gat_best_model.pt\", map_location=device))\n",
        "    evaluate_gat_model(model, test_loader, device, return_acc=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d9706cc-fd04-46f4-9ff5-7aef12f8a268",
      "metadata": {
        "id": "7d9706cc-fd04-46f4-9ff5-7aef12f8a268",
        "outputId": "a7504cbd-3aed-4355-c11a-20bd74f87948"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Fold 1/5 ===\n",
            "[Ep  1/50] Loss 167.1951 | Train Acc 0.4471 | Val Acc 0.4000\n",
            "  Best model updated!\n",
            "[Ep  2/50] Loss 90.0487 | Train Acc 0.4353 | Val Acc 0.4667\n",
            "  Best model updated!\n",
            "[Ep  3/50] Loss 61.4607 | Train Acc 0.5882 | Val Acc 0.2667\n",
            "[Ep  4/50] Loss 78.7587 | Train Acc 0.3882 | Val Acc 0.5333\n",
            "  Best model updated!\n",
            "[Ep  5/50] Loss 57.8758 | Train Acc 0.4706 | Val Acc 0.4667\n",
            "[Ep  6/50] Loss 54.4799 | Train Acc 0.4353 | Val Acc 0.4667\n",
            "[Ep  7/50] Loss 54.7765 | Train Acc 0.4824 | Val Acc 0.8000\n",
            "  Best model updated!\n",
            "[Ep  8/50] Loss 43.4903 | Train Acc 0.4706 | Val Acc 0.6000\n",
            "[Ep  9/50] Loss 41.5417 | Train Acc 0.4706 | Val Acc 0.7333\n",
            "[Ep 10/50] Loss 30.3839 | Train Acc 0.5294 | Val Acc 0.6000\n",
            "[Ep 11/50] Loss 24.4924 | Train Acc 0.5529 | Val Acc 0.8000\n",
            "[Ep 12/50] Loss 28.6295 | Train Acc 0.5647 | Val Acc 0.6000\n",
            "  Early stopping triggered.\n",
            "\n",
            "Best Validation Accuracy: 0.8000\n",
            "Fold 1  —  Test Acc: 0.500\n",
            " Confusion Matrix:\n",
            " [[4 9]\n",
            " [4 9]]\n",
            "\n",
            "=== Fold 2/5 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_324855/150056732.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(chk, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Ep  1/50] Loss 52.4649 | Train Acc 0.4471 | Val Acc 0.5000\n",
            "  Best model updated!\n",
            "[Ep  2/50] Loss 0.6817 | Train Acc 0.5059 | Val Acc 0.5000\n",
            "[Ep  3/50] Loss 0.7035 | Train Acc 0.4706 | Val Acc 0.5000\n",
            "[Ep  4/50] Loss 0.6741 | Train Acc 0.4941 | Val Acc 0.5000\n",
            "[Ep  5/50] Loss 0.6745 | Train Acc 0.5176 | Val Acc 0.5000\n",
            "[Ep  6/50] Loss 0.6785 | Train Acc 0.4706 | Val Acc 0.5000\n",
            "  Early stopping triggered.\n",
            "\n",
            "Best Validation Accuracy: 0.5000\n",
            "Fold 2  —  Test Acc: 0.520\n",
            " Confusion Matrix:\n",
            " [[ 0 12]\n",
            " [ 0 13]]\n",
            "\n",
            "=== Fold 3/5 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_324855/150056732.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(chk, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Ep  1/50] Loss 0.6933 | Train Acc 0.4471 | Val Acc 0.5000\n",
            "  Best model updated!\n",
            "[Ep  2/50] Loss 0.6768 | Train Acc 0.5059 | Val Acc 0.5000\n",
            "[Ep  3/50] Loss 0.6820 | Train Acc 0.4824 | Val Acc 0.5000\n",
            "[Ep  4/50] Loss 0.6814 | Train Acc 0.4941 | Val Acc 0.5000\n",
            "[Ep  5/50] Loss 0.6882 | Train Acc 0.4941 | Val Acc 0.5000\n",
            "[Ep  6/50] Loss 0.6820 | Train Acc 0.4941 | Val Acc 0.5000\n",
            "  Early stopping triggered.\n",
            "\n",
            "Best Validation Accuracy: 0.5000\n",
            "Fold 3  —  Test Acc: 0.520\n",
            " Confusion Matrix:\n",
            " [[ 0 12]\n",
            " [ 0 13]]\n",
            "\n",
            "=== Fold 4/5 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_324855/150056732.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(chk, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Ep  1/50] Loss 1.2772 | Train Acc 0.5059 | Val Acc 0.5000\n",
            "  Best model updated!\n",
            "[Ep  2/50] Loss 0.6772 | Train Acc 0.5176 | Val Acc 0.5000\n",
            "[Ep  3/50] Loss 0.6862 | Train Acc 0.5176 | Val Acc 0.5000\n",
            "[Ep  4/50] Loss 0.6994 | Train Acc 0.4471 | Val Acc 0.5000\n",
            "[Ep  5/50] Loss 0.6819 | Train Acc 0.5647 | Val Acc 0.5000\n",
            "[Ep  6/50] Loss 0.6835 | Train Acc 0.5176 | Val Acc 0.5000\n",
            "  Early stopping triggered.\n",
            "\n",
            "Best Validation Accuracy: 0.5000\n",
            "Fold 4  —  Test Acc: 0.480\n",
            " Confusion Matrix:\n",
            " [[ 0 13]\n",
            " [ 0 12]]\n",
            "\n",
            "=== Fold 5/5 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_324855/150056732.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(chk, map_location=device))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Ep  1/50] Loss 0.6957 | Train Acc 0.4588 | Val Acc 0.5000\n",
            "  Best model updated!\n",
            "[Ep  2/50] Loss 0.6842 | Train Acc 0.5059 | Val Acc 0.5000\n",
            "[Ep  3/50] Loss 0.6909 | Train Acc 0.5059 | Val Acc 0.5000\n",
            "[Ep  4/50] Loss 0.6475 | Train Acc 0.5176 | Val Acc 0.5000\n",
            "[Ep  5/50] Loss 0.6750 | Train Acc 0.5059 | Val Acc 0.5000\n",
            "[Ep  6/50] Loss 0.6837 | Train Acc 0.5059 | Val Acc 0.5000\n",
            "  Early stopping triggered.\n",
            "\n",
            "Best Validation Accuracy: 0.5000\n",
            "Fold 5  —  Test Acc: 0.480\n",
            " Confusion Matrix:\n",
            " [[ 0 13]\n",
            " [ 0 12]]\n",
            "\n",
            "=== CV Results (5-fold) ===\n",
            "Accuracy: 0.500 ± 0.018\n",
            "Aggregated Confusion Matrix (summing folds):\n",
            " [[ 4 59]\n",
            " [ 4 59]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_324855/150056732.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(chk, map_location=device))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# ← make sure train_gat_model, evaluate_gat_model,\n",
        "#    MultimodalGATDataset and GATFusion are imported/defined above\n",
        "\n",
        "def cross_validate_gat(\n",
        "    df: pd.DataFrame,\n",
        "    n_splits: int = 5,\n",
        "    epochs: int = 50,\n",
        "    lr: float = 5e-4,\n",
        "    patience: int = 5,\n",
        "    use_focal: bool = False,\n",
        "    class0_weight: float = 1.1,\n",
        "    batch_size: int = 16,\n",
        "):\n",
        "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    fold_accs = []\n",
        "    fold_cms  = []\n",
        "\n",
        "    X = df.index.values\n",
        "    y = df[\"label\"].values\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):\n",
        "        print(f\"\\n=== Fold {fold}/{n_splits} ===\")\n",
        "        # Build train+val vs test\n",
        "        df_train_full = df.iloc[train_idx].reset_index(drop=True)\n",
        "        df_test       = df.iloc[test_idx].reset_index(drop=True)\n",
        "\n",
        "        # Inner split: train vs val for early stopping\n",
        "        train_df, val_df = train_test_split(\n",
        "            df_train_full,\n",
        "            test_size=0.15,\n",
        "            stratify=df_train_full[\"label\"],\n",
        "            random_state=42,\n",
        "        )\n",
        "\n",
        "        # DataLoaders\n",
        "        train_loader = DataLoader(\n",
        "            MultimodalGATDataset(train_df),\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            MultimodalGATDataset(val_df),\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            MultimodalGATDataset(df_test),\n",
        "            batch_size=batch_size,\n",
        "        )\n",
        "\n",
        "        # Init a fresh model\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = GATFusion(\n",
        "            eeg_dim=len(eeg_features),\n",
        "            eye_dim=len(eye_features),\n",
        "            beh_dim=len(beh_features),\n",
        "            hidden_dim=64,\n",
        "            out_dim=64\n",
        "        ).to(device)\n",
        "\n",
        "        # Train + early‑stop on val\n",
        "        train_gat_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            val_loader,\n",
        "            device,\n",
        "            epochs=epochs,\n",
        "            lr=lr,\n",
        "            patience=patience,\n",
        "            use_focal=use_focal,\n",
        "            class0_weight=class0_weight,\n",
        "        )\n",
        "\n",
        "        # Load best weights & evaluate on this fold’s test\n",
        "        chk = \"results/gat_best_model.pt\"\n",
        "        model.load_state_dict(torch.load(chk, map_location=device))\n",
        "\n",
        "        # Collect preds / labels\n",
        "        all_preds, all_labels = [], []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for eeg, eye, beh, labels, _ in test_loader:\n",
        "                eeg, eye, beh = eeg.to(device), eye.to(device), beh.to(device)\n",
        "                out = model(eeg, eye, beh)\n",
        "                all_preds.extend(out.argmax(1).cpu().numpy())\n",
        "                all_labels.extend(labels.numpy())\n",
        "\n",
        "        acc = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "        cm  = confusion_matrix(all_labels, all_preds, labels=[0,1])\n",
        "\n",
        "        print(f\"Fold {fold}  —  Test Acc: {acc:.3f}\")\n",
        "        print(\" Confusion Matrix:\\n\", cm)\n",
        "\n",
        "        fold_accs.append(acc)\n",
        "        fold_cms.append(cm)\n",
        "\n",
        "    # Summary\n",
        "    mean_acc = np.mean(fold_accs)\n",
        "    std_acc  = np.std(fold_accs)\n",
        "    print(f\"\\n=== CV Results ({n_splits}-fold) ===\")\n",
        "    print(f\"Accuracy: {mean_acc:.3f} ± {std_acc:.3f}\")\n",
        "\n",
        "    # aggregated confusion matrix (sum over folds)\n",
        "    agg_cm = sum(fold_cms)\n",
        "    print(\"Aggregated Confusion Matrix (summing folds):\\n\", agg_cm)\n",
        "\n",
        "\n",
        "# ── USAGE ──\n",
        "if __name__ == \"__main__\":\n",
        "    cross_validate_gat(\n",
        "        balanced_df,\n",
        "        n_splits=5,\n",
        "        epochs=50,\n",
        "        lr=5e-4,\n",
        "        patience=5,\n",
        "        use_focal=False,      # or True\n",
        "        class0_weight=1.0,    # tune as you like\n",
        "        batch_size=16\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "363882be-deef-4e50-afdd-4319cec769f9",
      "metadata": {
        "id": "363882be-deef-4e50-afdd-4319cec769f9",
        "outputId": "ecabf677-8d60-442e-da27-3ad5f369bab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogReg 5‑fold Acc: 0.555 ± 0.120\n",
            "RandomForest 5‑fold Acc: 0.746 ± 0.037\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. Pull out features & labels from your balanced_df\n",
        "feature_cols = [c for c in balanced_df.columns if c.startswith((\"eeg_\", \"eye_\", \"beh_\"))]\n",
        "X = balanced_df[feature_cols].values\n",
        "y = balanced_df[\"label\"].values\n",
        "\n",
        "# 2. Stratified 5‑fold splitter\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 3. Logistic Regression baseline\n",
        "lr = LogisticRegression(solver=\"liblinear\")\n",
        "lr_scores = cross_val_score(lr, X, y, cv=skf, scoring=\"accuracy\")\n",
        "print(f\"LogReg 5‑fold Acc: {lr_scores.mean():.3f} ± {lr_scores.std():.3f}\")\n",
        "\n",
        "# 4. Random Forest baseline\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
        "rf_scores = cross_val_score(rf, X, y, cv=skf, scoring=\"accuracy\")\n",
        "print(f\"RandomForest 5‑fold Acc: {rf_scores.mean():.3f} ± {rf_scores.std():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e066871-96bd-4f21-a543-5373061d4a9c",
      "metadata": {
        "id": "1e066871-96bd-4f21-a543-5373061d4a9c",
        "outputId": "5be330e8-d5bc-4f53-a3c0-283bbac5dff9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nThe GATFusion model you built first independently projects each modality’s 13 summary features (EEG, eye–tracking, behavioral) into a shared 64‑dim space, treats those three embeddings as nodes in a tiny fully connected graph, applies a single graph‐attention layer, and then classifies the fused output via a two‐layer MLP. Despite careful balancing, focal‐loss weighting, and both single‐split and 5‑fold cross‑validation, the network never learned to separate the classes: average CV accuracy hovered at 50\\xa0% (±\\xa01.8\\xa0%), with the aggregated confusion matrix showing virtually random predictions. In contrast, a simple Random Forest trained on the same features achieved approximately 75\\xa0% accuracy under 5‑fold CV, demonstrating that the summary statistics do contain discriminative signal but that the GAT architecture and small sample size are a poor fit. To move forward, it’s best either to pivot to and tune the tree‐ensemble baseline or to engineer richer modality‐specific features before revisiting deep fusion models.\\n\\n\\n\\n\\n\\n'"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "The GATFusion model you built first independently projects each modality’s 13 summary features (EEG, eye–tracking, behavioral) into a shared 64‑dim space, treats those three embeddings as nodes in a tiny fully connected graph, applies a single graph‐attention layer, and then classifies the fused output via a two‐layer MLP. Despite careful balancing, focal‐loss weighting, and both single‐split and 5‑fold cross‑validation, the network never learned to separate the classes: average CV accuracy hovered at 50 % (± 1.8 %), with the aggregated confusion matrix showing virtually random predictions. In contrast, a simple Random Forest trained on the same features achieved approximately 75 % accuracy under 5‑fold CV, demonstrating that the summary statistics do contain discriminative signal but that the GAT architecture and small sample size are a poor fit. To move forward, it’s best either to pivot to and tune the tree‐ensemble baseline or to engineer richer modality‐specific features before revisiting deep fusion models.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b3e4d58-0e30-4bdf-85fc-0ef1b2cebc13",
      "metadata": {
        "id": "9b3e4d58-0e30-4bdf-85fc-0ef1b2cebc13"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (jupyter_env)",
      "language": "python",
      "name": "jupyter_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}